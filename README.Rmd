---
output: github_document
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "100%"
)
```

# MinML

<!-- badges: start -->
<!-- badges: end -->

The MinML package is a lightweight and modular machine learning framework for R, created for learning how to implement a ML package from the ground up in R for more customizbility. 

## Installation

You can install the development version of MinML from [GitHub](https://github.com/) with:

``` r
# install.packages("pak")
pak::pak("Hek7/MinML")
```

## Example

### Example on how to build and train a model in MinML

```{r example}
library(MinML)

# Dummy normal Data 
X <- matrix(rnorm(50), nrow = 10, ncol = 5)

# Dummy targets 
y_true <- matrix(rnorm(10), nrow = 10, ncol = 1)

# Define model with linear and activations
model <- Sequential(
  Linear(input_features = 5, output_features = 10),
  activation_relu,
  Linear(input_features = 10, output_features = 1),
  activation_sigmoid
)

# Forward pass returns predictions for current weights
y_pred <- model$forward(X)

cat("Prediction dimensions:", dim(y_pred), "\n")

# Calculate the loss other losses to get added
loss <- squared_loss(y_true, y_pred)
cat("Current MSE Loss:", loss, "\n")

# Backward pass to get the grad loss 
grad_loss <- squared_loss_grad(y_true, y_pred)

# populates the weight and biases grads 
model$backward(grad_loss)

#Some getters for getting the grads 
all_grads <- model$get_grads()
```

### Example: Training Loop

```{r training example}
epochs <- 100
learning_rate <- 0.01

# Initialize a vector to store loss history
loss_history <- numeric(epochs)

for (epoch in 1:epochs) {
  
  # Forward pass 
  y_pred <- model$forward(X)
  
  # Compute Loss
  loss <- squared_loss(y_true, y_pred)
  
  # Store loss in history
  loss_history[epoch] <- loss
  
  # Backward pass 
  grad_loss <- squared_loss_grad(y_true, y_pred)
  
  # Backprop for grads 
  model$backward(grad_loss)
  
  # Optimize using SGD  
  for (i in seq_along(model$modules)) {
    m <- model$modules[[i]]
    
    # Only update layers that have params
    if ("get_params" %in% names(m) && "get_grads" %in% names(m)) {
      
      # Get current params & grads
      params <- m$get_params()
      grads  <- m$get_grads()
      
      # Compute updated params
      updated <- sgd_update_params(params, grads, learning_rate)
      
      m$set_params(updated)
    
      }
    }

  # Print progress every 10 epochs
  if (epoch %% 10 == 0) {
    cat(sprintf("Epoch %d/%d - Loss: %.6f\n", epoch, epochs, loss))
  }
}

cat("Training complete.\n")
```


### Different Activation Functions

Other activation functions:

```{r activations}
# Create models with different activations
model_sigmoid <- Sequential(
  Linear(2, 16),
  activation_sigmoid,
  Linear(16, 1)
)

model_tanh <- Sequential(
  Linear(2, 16),
  activation_tanh,
  Linear(16, 1)
)

model_gelu <- Sequential(
  Linear(2, 16),
  activation_gelu,
  Linear(16, 1)
)
```

