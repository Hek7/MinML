% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Activations.R
\name{activation_leaky_relu_grad}
\alias{activation_leaky_relu_grad}
\title{Leaky ReLU Backward Pass (Derivative)}
\usage{
activation_leaky_relu_grad(x, grad_output, alpha = 0.01)
}
\arguments{
\item{x}{The original input to the forward pass.}

\item{grad_output}{The gradient of the loss with respect to the output.}

\item{alpha}{The slope for negative inputs (default: 0.01).}

\item{output}{The output of the forward pass (ignored, only input $x$ is needed for derivative calculation).}
}
\value{
The gradient with respect to the input, $x$.
}
\description{
Computes the derivative of the Leaky ReLU function: $f'(x) = 1$ if $x > 0$, $\alpha$ otherwise.
}
