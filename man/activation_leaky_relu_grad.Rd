% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Activations.R
\name{activation_leaky_relu_grad}
\alias{activation_leaky_relu_grad}
\title{Leaky ReLU Backward Pass (Derivative)}
\usage{
activation_leaky_relu_grad(output, grad_output, alpha = 0.01)
}
\arguments{
\item{output}{Output of the forward pass.}

\item{grad_output}{Gradient of the loss w.r.t the output.}

\item{alpha}{Slope for negative inputs (default 0.01).}
}
\value{
Gradient w.r.t the input, as a matrix.
}
\description{
Computes derivative of Leaky ReLU: \eqn{f'(x) = 1} if x > 0, \eqn{\alpha} otherwise.
}
