% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/Activations.R
\name{activation_gelu_grad}
\alias{activation_gelu_grad}
\title{GeLU Backward Pass (Derivative)}
\usage{
activation_gelu_grad(x, grad_output)
}
\arguments{
\item{x}{The original input to the forward pass.}

\item{grad_output}{The gradient of the loss with respect to the output.}
}
\value{
The gradient with respect to the input, $x$.
}
\description{
Computes the derivative of the GeLU function: $f'(x) = \Phi(x) + x \cdot \phi(x)$,
where $\phi(x)$ is the probability density function (PDF) of the standard normal distribution.
}
